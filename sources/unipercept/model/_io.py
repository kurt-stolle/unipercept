r"""
Definitions of the input and outputs of a model in the UniPercept framework.
"""

from __future__ import annotations

import pprint
import typing as T
from dataclasses import field

import torch
from tensordict import TensorDict, TensorDictBase
from torch import Tensor

from unipercept.data import tensors
from unipercept.utils.tensorclass import Tensorclass

__all__ = ["CaptureData", "MotionData", "InputData"]

#########################
# STRUCTURED INPUT DATA #
#########################


class CaptureData(Tensorclass):
    """A capture describes a single frame of data, including images, label maps, depth maps, and camera parameters."""

    times: Tensor
    images: tensors.Image
    segmentations: tensors.PanopticMap | None = field(
        default=None, metadata={"help": "Panoptic segmentation maps for the capture."}
    )
    depths: tensors.DepthMap | None = field(
        default=None, metadata={"help": "Depth maps for the capture."}
    )
    metadata: TensorDict | None = field(
        default=None,
        metadata={
            "help": (
                "Additional metadata for the capture. "
                "Can be used to store arbitrary information about the capture, e.g. generated by a preprocessing step."
            )
        },
    )

    @property
    def num_frames(self) -> int:
        """
        Return the amount of frames (i.e. pair dimension of captures) in the input data.
        """
        return self.images.shape[-4]  # tensors.Images are (..., F, 3, H, W)

    def __post_init__(self):
        assert (
            self.images.ndim >= 3 and self.images.shape[-3] == 3
        ), f"tensors.Images must be of shape (..., 3, H, W), got {self.images.shape}"

    def fix_subtypes_(self) -> T.Self:
        """Subtypes are removed when converting to a tensor, so this method restores them."""

        self.images = self.images.as_subclass(tensors.Image)
        self.segmentations = (
            self.segmentations.as_subclass(tensors.PanopticMap)
            if self.segmentations is not None
            else None
        )
        self.depths = (
            self.depths.as_subclass(tensors.DepthMap)
            if self.depths is not None
            else None
        )
        return self

    def items(self):
        for k, v in self._tensordict.items():
            if v is None:
                continue

            # HACK: This fixes the issue in tensordict where subclasses are removed for unknown reasons.
            if k == "images":
                v = v.as_subclass(tensors.Image)
            elif k == "segmentations":
                v = v.as_subclass(tensors.PanopticMap)
            elif k == "depths":
                v = v.as_subclass(tensors.DepthMap)

            yield k, v

    @property
    def height(self) -> int:
        """Returns the height of the image."""

        return self.images.shape[-2]

    @property
    def width(self) -> int:
        """Returns the width of the image."""

        return self.images.shape[-1]

    def fillna(self, inplace=True) -> T.Self:
        """
        See :meth:`InputData.fillna`

        Notes
        -----
        The default shape is inferred from the shape of the images attribute, which may never be ``None`` by
        definition.
        """
        has_panoptic = self.segmentations is not None
        has_depth = self.depths is not None
        if has_panoptic and has_depth:
            return self

        if not inplace:
            self = self.clone()
        shape = torch.Size([*self.batch_size, self.height, self.width])
        device = self.images.device

        if not has_panoptic:
            self.segmentations = tensors.PanopticMap.default(shape, device=device)
        if not has_depth:
            self.depths = tensors.DepthMap.default(shape, device=device)

        return self


class MotionData(Tensorclass):
    """Data describing motion between time steps."""

    optical_flow: tensors.OpticalFlow | None
    transform: Tensor
    metadata: TensorDict | None = field(
        default=None,
        metadata={"help": "Additional metadata for the motion. "},
    )

    def __post_init__(self):
        if self.optical_flow is not None and (
            self.optical_flow.ndim != len(self.batch_size) + 3
            or self.optical_flow.shape[-3] != 2
        ):
            raise ValueError("Optical flows must be of shape (..., 2, H, W)")
        if self.transform is not None and (
            self.transform.ndim != len(self.batch_size) + 1
            or self.transform.shape[-1] != 4
        ):
            raise ValueError("transform must be of shape (..., 4)")


class InputData(Tensorclass):
    """Describes the input data to any model in the UniPercept framework."""

    ids: Tensor = field(
        metadata={
            "shape": ["B", 2],
            "help": (
                "Unique IDs for each item in the batch, consisting of a group ID and item ID, usually encoding "
                "information like sequence and frame number during inference time."
            ),
        }
    )
    captures: CaptureData = field(
        metadata={
            "shape": ["B", "F"],  # batch, num_frames_per_capture
            "help": (
                "Capture data for each frame in the batch. The first dimension is the batch dimension, the second "
                "dimension is the frame/pair dimension."
            ),
            "tensorclass": CaptureData,
        }
    )
    cameras: tensors.PinholeCamera = field(
        default=None,  # type: ignore
        metadata={
            "shape": ["B", "N"],  # batch, num_cameras_per_capture
            "help": "Camera parameters for each capture item in the batch.",
        },
    )
    motions: MotionData | None = field(
        default=None,
        metadata={
            "shape": ["B", "F"],
            "help": "Motion data between frames, if available. The motion at the first frame is by definition zero.",
            "tensorclass": MotionData,
        },
    )
    metadata: TensorDict | None = field(
        default=None,
        metadata={"help": "Additional metadata for the input data."},
    )

    def __post_init__(self):
        if self.cameras is None:
            self.cameras = tensors.PinholeCamera.with_defaults_as(self.captures.images)

    @property
    def group_id(self) -> Tensor:
        """Returns the group ID of the input data."""
        return self.ids[..., 0]

    @property
    def item_id(self) -> Tensor:
        """Returns the item ID of the input data."""
        return self.ids[..., 1]

    @property
    def num_frames(self) -> int:
        """
        Return the amount of frames (i.e. pair dimension of captures) in the input data.
        """
        return self.captures.num_frames

    def extract_frame(self, index: int) -> T.Self:
        """
        Return a single frame of the input data, resulting in an input data object with only a single capture and no
        motions.

        Parameters
        ----------
        index : int
            The index of the frame to extract.
        Returns
        -------
        InputData
            The input data with only the extracted frame.
        """
        if index < 0 or index >= self.num_frames:
            raise ValueError(
                f"Index must be between 0 and {self.num_frames}, got {index}"
            )

        return self.__class__(
            ids=self.ids.clone(),
            captures=self.captures[..., index].clone(),
            motions=None,
            cameras=self.cameras.clone().as_subclass(tensors.PinholeCamera),
            batch_size=self.batch_size,
        )

    @classmethod
    def collate[_Q](
        cls, items: T.Sequence[tuple[_Q, T.Self]]
    ) -> tuple[list[_Q], TensorDictBase]:
        """
        Collates a batch of input data into a single input data object.
        """

        sources, batch = zip(*items, strict=False)

        if len(batch) == 0:
            raise ValueError("Batch must be non-empty!")

        assert all(
            len(b.batch_size) == 0 for b in batch
        ), f"Batch size must be 0 for all inputs! Got: {[b.batch_size for b in batch]}"

        try:
            # return LazyStackedTensorDict(*batch)
            return sources, torch.stack(batch)  # type: ignore
        except RuntimeError as e:
            msg = (
                f"Failed to collate batch of input data: {e}. "
                f"Ensure that all input data objects have the same shape. \n\n"
                f"Batch sizes: {[b.shape for b in batch]}. \n\n"
                f"Batch items: \n{pprint.pformat(batch)}"
            )
            raise ValueError(msg) from e
        # batch = [b.to_tensordict() for b in batch]
        # return LazyStackedTensorDict(*batch)

    def fillna(self, inplace=True) -> T.Self:
        """
        Materializes the default values for any attribute that is ``None``, i.e. when no data is available for that
        entry.
        """
        if not inplace:
            self = self.clone()

        if self.captures is not None:
            self.captures.fillna(inplace=True)
        # if self.motions is not None:
        #     self.motions.fillna(inplace=True)
        # if self.cameras is not None:
        #     self.cameras.fillna(inplace=True)

        return self
