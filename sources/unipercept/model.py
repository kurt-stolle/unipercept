"""
Defines the interface for a perception model.
"""

from __future__ import annotations

import abc
import os
import typing as T
from dataclasses import field

import torch
import torch.nn as nn
import typing_extensions as TX
from tensordict import LazyStackedTensorDict, TensorDict, TensorDictBase
from torch.utils._pytree import TreeSpec, tree_flatten, tree_unflatten

from unipercept import file_io
from unipercept.data.tensors import DepthMap, Image, OpticalFlow, PanopticMap
from unipercept.log import get_logger
from unipercept.utils.tensorclass import Tensorclass

_logger = get_logger(__name__)

__all__ = []

#########################
# STRUCTURED INPUT DATA #
#########################


class CaptureData(Tensorclass):
    """A capture describes a single frame of data, including images, label maps, depth maps, and camera parameters."""

    times: torch.Tensor
    images: Image
    segmentations: PanopticMap | None = field(
        default=None, metadata={"help": "Panoptic segmentation maps for the capture."}
    )
    depths: DepthMap | None = field(
        default=None, metadata={"help": "Depth maps for the capture."}
    )
    metadata: TensorDict | None = field(
        default=None,
        metadata={
            "help": (
                "Additional metadata for the capture. "
                "Can be used to store arbitrary information about the capture, e.g. generated by a preprocessing step."
            )
        },
    )

    @property
    def num_frames(self) -> int:
        """
        Return the amount of frames (i.e. pair dimension of captures) in the input data.
        """
        return self.images.shape[-4]  # Images are (..., F, 3, H, W)

    def __post_init__(self):
        assert (
            self.images.ndim >= 3 and self.images.shape[-3] == 3
        ), f"Images must be of shape (..., 3, H, W), got {self.images.shape}"

    def fix_subtypes_(self) -> T.Self:
        """Subtypes are removed when converting to a tensor, so this method restores them."""

        self.images = self.images.as_subclass(Image)
        self.segmentations = (
            self.segmentations.as_subclass(PanopticMap)
            if self.segmentations is not None
            else None
        )
        self.depths = (
            self.depths.as_subclass(DepthMap) if self.depths is not None else None
        )
        return self

    def items(self):
        for k, v in self._tensordict.items():
            if v is None:
                continue

            # HACK: This fixes the issue in tensordict where subclasses are removed for unknown reasons.
            if k == "images":
                v = v.as_subclass(Image)
            elif k == "segmentations":
                v = v.as_subclass(PanopticMap)
            elif k == "depths":
                v = v.as_subclass(DepthMap)

            yield k, v

    @property
    def height(self) -> int:
        """Returns the height of the image."""

        return self.images.shape[-2]

    @property
    def width(self) -> int:
        """Returns the width of the image."""

        return self.images.shape[-1]

    def fillna(self, inplace=True) -> T.Self:
        """
        Materializes the default values for any attribute that is ``None``, i.e. when no data is available for that
        entry. The default shape is inferred from the shape of the images attribute, which may never be ``None`` by
        definition.
        """
        has_panoptic = self.segmentations is not None
        has_depth = self.depths is not None
        if has_panoptic and has_depth:
            return self

        if not inplace:
            self = self.clone()
        shape = torch.Size([*self.batch_size, self.height, self.width])
        device = self.images.device

        if not has_panoptic:
            self.segmentations = PanopticMap.default(shape, device=device)
        if not has_depth:
            self.depths = DepthMap.default(shape, device=device)

        return self


class CameraModel(Tensorclass):
    """
    Build pinhole camera calibration matrices.

    See: https://kornia.readthedocs.io/en/latest/geometry.camera.pinhole.html
    """

    image_size: torch.Tensor  # shape: ..., 2 (height, width)
    matrix: torch.Tensor  # shape: (... x 4 x 4) K
    pose: torch.Tensor  # shape: (... x 4 x 4) Rt

    @property
    def height(self) -> torch.Tensor:
        return self.image_size[..., 0]

    @property
    def width(self) -> torch.Tensor:
        return self.image_size[..., 1]

    @TX.override
    def __post_init__(self):
        if self.matrix.shape[-2:] != (4, 4):
            raise ValueError("Camera matrix must be of shape (..., 4, 4)")
        if self.pose.shape[-2:] != (4, 4):
            raise ValueError("Camera pose must be of shape (..., 4, 4)")
        if self.image_size.shape[-1] != 2:
            raise ValueError("Camera size must be of shape (..., 2)")


class MotionData(Tensorclass):
    """Data describing motion between time steps."""

    optical_flow: OpticalFlow | None
    transform: torch.Tensor

    def __post_init__(self):
        if self.optical_flow is not None and (
            self.optical_flow.ndim != len(self.batch_size) + 3
            or self.optical_flow.shape[-3] != 2
        ):
            raise ValueError("Optical flows must be of shape (..., 2, H, W)")
        if self.transform is not None and (
            self.transform.ndim != len(self.batch_size) + 1
            or self.transform.shape[-1] != 4
        ):
            raise ValueError("transform must be of shape (..., 4)")


class InputData(Tensorclass):
    """Describes the input data to any model in the UniPercept framework."""

    ids: torch.Tensor = field(
        metadata={
            "shape": ["B", 2],
            "help": (
                "Unique IDs for each item in the batch, consisting of a group ID and item ID, usually encoding "
                "information like sequence and frame number during inference time."
            ),
        }
    )
    captures: CaptureData = field(
        metadata={
            "shape": ["B", "F"],
            "help": (
                "Capture data for each frame in the batch. The first dimension is the batch dimension, the second "
                "dimension is the frame/pair dimension."
            ),
            "tensorclass": CaptureData,
        }
    )
    motions: MotionData | None = field(
        default=None,
        metadata={
            "shape": ["B", "F"],
            "help": "Motion data between frames, if available. The motion at the first frame is by definition zero.",
            "tensorclass": MotionData,
        },
    )
    cameras: CameraModel | None = field(
        default=None,
        metadata={
            "shape": ["B"],
            "help": "Camera parameters for each capture item in the batch.",
            "tensorclass": CameraModel,
        },
    )
    content_boxes: torch.Tensor | None = field(
        default=None,
        metadata={
            "shape": ["B", 4],
            "help": (
                "Bounding boxes that describe the content of the image. "
                "Useful in cases where some of the images in a batch "
                "are padded, e.g. when using a sampler that samples from a dataset with images of different sizes."
            ),
        },
    )

    @property
    def num_frames(self) -> int:
        """
        Return the amount of frames (i.e. pair dimension of captures) in the input data.
        """
        return self.captures.num_frames

    def extract_frame(self, index: int) -> T.Self:
        """
        Return a single frame of the input data, resulting in an input data object with only a single capture and no
        motions.

        Parameters
        ----------
        index : int
            The index of the frame to extract.
        Returns
        -------
        InputData
            The input data with only the extracted frame.
        """
        if index < 0 or index >= self.num_frames:
            raise ValueError(
                f"Index must be between 0 and {self.num_frames}, got {index}"
            )

        return self.__class__(
            ids=self.ids.clone(),
            captures=self.captures[..., index].clone(),
            motions=None,
            cameras=self.cameras.clone(),
            content_boxes=self.content_boxes.clone(),
            batch_size=self.batch_size,
        )

    @classmethod
    def collate(cls, batch: T.Sequence[T.Self]) -> LazyStackedTensorDict:
        """
        Collates a batch of input data into a single input data object.
        """

        if len(batch) == 0:
            raise ValueError("Batch must be non-empty!")

        assert all(
            len(b.batch_size) == 0 for b in batch
        ), f"Batch size must be 0 for all inputs! Got: {[b.batch_size for b in batch]}"

        return torch.stack(batch)  # type: ignore
        # batch = [b.to_tensordict() for b in batch]
        # return LazyStackedTensorDict(*batch)


#########################
# BASE CLASS FOR MODELS #
#########################

ModelInput = InputData | TensorDictBase | T.Dict[str, torch.Tensor]
ModelOutput = TensorDictBase


class ModelBase(nn.Module):
    """
    Defines the interface for a perception model. Defines the interface used throughout `unipercept`.

    Notes
    -----
    This class is abstract and cannot be instantiated directly. Instead, use :class:`ModelFactory` to instantiate a
    model from a configuration.

    Additionally, while this package defines a structured input data format, models are free to define their
    own, the interface only requires that the inptus and outputs are instances of a :class:`tensordict.TensorDictBase`
    subclass.
    """

    def __init__(self):
        super().__init__()

    if T.TYPE_CHECKING:

        @abc.abstractmethod
        @TX.override
        def forward(self, inputs: ModelInput) -> ModelOutput:
            ...

        @TX.override
        def __call__(self, inputs: ModelInput) -> ModelOutput:
            ...


class ModelFactory:
    def __init__(
        self,
        model_config,
        checkpoint_path: file_io.Path | os.PathLike | str | None = None,
    ):
        self.model_config = model_config
        self.checkpoint_path = checkpoint_path

    def __call__(self, *args, **kwargs) -> ModelBase:
        """
        TODO interface not clearly defined yet
        """
        from unipercept import load_checkpoint
        from unipercept.config import instantiate

        model = T.cast(ModelBase, instantiate(self.model_config))

        if self.checkpoint_path is not None:
            _logger.info("Loading model weights from %s", self.checkpoint_path)
            load_checkpoint(self.checkpoint_path, model)
        else:
            _logger.info("No model weights checkpoint path provided, skipping recovery")

        return model


class ModelAdapter(nn.Module):
    """
    A model may take rich input/output format (e.g. dict or custom classes),
    but `torch.jit.trace` requires tuple of tensors as input/output.
    This adapter flattens input/output format of a model so it becomes traceable.

    Notes
    -----
    This implementation is based on the Detectron2 ``TracingAdapter`` class. We use a custom implementation because
    Detectron2's implementation is not compatible with PyTree-based flattening.
    """

    flattened_inputs: T.Tuple[torch.Tensor, ...]
    inputs_schema: TreeSpec | None
    outputs_schema: TreeSpec | None

    def __init__(
        self,
        model: nn.Module,
        inputs,
        inference_func: T.Optional[T.Callable] = None,
        allow_non_tensor: bool = False,
    ):
        """
        Parameters
        ----------

        model
            An nn.Module
        inputs
            An input argument or a tuple of input arguments used to call model.
            After flattening, it has to only consist of tensors.
        inference_func
            A callable that takes (model, *inputs), calls the
            model with inputs, and return outputs. By default it
            is ``lambda model, *inputs: model(*inputs)``. Can be override
            if you need to call the model differently.
        allow_non_tensor
            Allow inputs/outputs to contain non-tensor objects.
            This option will filter out non-tensor objects to make the
            model traceable, but ``inputs_schema``/``outputs_schema`` cannot be
            used anymore because inputs/outputs cannot be rebuilt from pure tensors.
            This is useful when you're only interested in the single trace of
            execution (e.g. for flop count), but not interested in
            generalizing the traced graph to new inputs.
        """

        super().__init__()

        if isinstance(
            model, (nn.parallel.distributed.DistributedDataParallel, nn.DataParallel)
        ):
            model = model.module
        self.model = model
        if not isinstance(inputs, tuple):
            inputs = (inputs,)
        self.inputs = inputs
        self.allow_non_tensor = allow_non_tensor
        if inference_func is None:
            inference_func = lambda model, *inputs: model(*inputs)  # noqa
        self.inference_func = inference_func
        inputs_flat, inputs_spec = tree_flatten(inputs)

        if not all(isinstance(x, torch.Tensor) for x in inputs_flat):
            if self.allow_non_tensor:
                inputs_flat = [x for x in inputs_flat if isinstance(x, torch.Tensor)]
                inputs_spec = None
            else:
                for input in inputs_flat:
                    if isinstance(input, torch.Tensor):
                        continue
                    raise ValueError(
                        "Inputs for tracing must only contain tensors. "
                        f"Got a {type(input)} instead."
                    )

        self.flattened_inputs = tuple(inputs_flat)  # type: ignore
        self.inputs_schema = inputs_spec

    @TX.override
    def forward(self, *args: torch.Tensor):
        with torch.inference_mode():
            if self.inputs_schema is not None:
                inputs_orig_format = tree_unflatten(list(args), self.inputs_schema)
            else:
                if args != self.flattened_inputs:
                    raise ValueError(
                        "TracingAdapter does not contain valid inputs_schema."
                        " So it cannot generalize to other inputs and must be"
                        " traced with `.flattened_inputs`."
                    )
                inputs_orig_format = self.inputs

            outputs = self.inference_func(self.model, *inputs_orig_format)
            flattened_outputs, schema = tree_flatten(outputs)

            flattened_output_tensors = tuple(
                [x for x in flattened_outputs if isinstance(x, torch.Tensor)]
            )
            if len(flattened_output_tensors) < len(flattened_outputs):
                if self.allow_non_tensor:
                    flattened_outputs = flattened_output_tensors
                    self.outputs_schema = None
                else:
                    raise ValueError(
                        "Model cannot be traced because some model outputs "
                        "cannot flatten to tensors."
                    )
            else:
                if self.outputs_schema is None:
                    self.outputs_schema = schema
                else:
                    assert self.outputs_schema == schema, (
                        "Model should always return outputs with the same "
                        "structure so it can be traced!"
                    )
            return flattened_outputs

    def _create_wrapper(self, traced_model):
        """
        Return a function that has an input/output interface the same as the
        original model, but it calls the given traced model under the hood.
        """

        def forward(*args):
            assert self.outputs_schema is not None

            flattened_inputs, _ = tree_flatten(args)
            flattened_outputs = traced_model(*flattened_inputs)
            return tree_unflatten(flattened_outputs, self.outputs_schema)

        return forward
